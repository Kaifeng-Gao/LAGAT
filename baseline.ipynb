{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch import nn\n",
    "from torch_geometric.utils import from_dgl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.fraud_dataset as fraud_dataset\n",
    "\n",
    "DATASET_NAME = \"yelp\"\n",
    "TRAIN_SIZE = 0.4\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_SEED = 42\n",
    "FORCE_RELOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method HeteroData.metadata of HeteroData(\n",
       "  review={\n",
       "    test_mask=[45954],\n",
       "    val_mask=[45954],\n",
       "    train_mask=[45954],\n",
       "    label=[45954],\n",
       "    feature=[45954, 32],\n",
       "  },\n",
       "  (review, net_rsr, review)={ edge_index=[2, 6805486] },\n",
       "  (review, net_rtr, review)={ edge_index=[2, 1147232] },\n",
       "  (review, net_rur, review)={ edge_index=[2, 98630] }\n",
       ")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data = fraud_dataset.FraudDataset(\n",
    "    DATASET_NAME, \n",
    "    train_size=TRAIN_SIZE, \n",
    "    val_size=VAL_SIZE, \n",
    "    random_seed=RANDOM_SEED, \n",
    "    force_reload=FORCE_RELOAD\n",
    ")\n",
    "graph = fraud_data[0]\n",
    "\n",
    "data = from_dgl(graph)\n",
    "data.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, dropout=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "        # First GAT convolution layer\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout, bias=False)\n",
    "        # Second GAT convolution layer\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=True, bias=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))  # Apply the first GATConv layer\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)  # Apply the second GATConv layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name        target                                args                     kwargs\n",
      "-------------  ----------  ------------------------------------  -----------------------  ----------------------------------------------\n",
      "placeholder    x           x                                     ()                       {}\n",
      "placeholder    edge_index  edge_index                            ()                       {}\n",
      "call_function  dropout     <function dropout at 0x7f35d19f3ce0>  (x,)                     {'p': 0.6, 'training': True, 'inplace': False}\n",
      "call_module    conv1       conv1                                 (dropout, edge_index)    {}\n",
      "call_function  elu         <function elu at 0x7f35d1a00400>      (conv1,)                 {'alpha': 1.0, 'inplace': False}\n",
      "call_function  dropout_1   <function dropout at 0x7f35d19f3ce0>  (elu,)                   {'p': 0.6, 'training': True, 'inplace': False}\n",
      "call_module    conv2       conv2                                 (dropout_1, edge_index)  {}\n",
      "output         output      output                                (conv2,)                 {}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x, edge_index):\n",
      "    dropout = torch.nn.functional.dropout(x, p = 0.6, training = True, inplace = False);  x = None\n",
      "    conv1 = self.conv1(dropout, edge_index);  dropout = None\n",
      "    elu = torch.nn.functional.elu(conv1, alpha = 1.0, inplace = False);  conv1 = None\n",
      "    dropout_1 = torch.nn.functional.dropout(elu, p = 0.6, training = True, inplace = False);  elu = None\n",
      "    conv2 = self.conv2(dropout_1, edge_index);  dropout_1 = edge_index = None\n",
      "    return conv2\n",
      "    \n",
      "opcode         name                                 target                                                  args                                                      kwargs\n",
      "-------------  -----------------------------------  ------------------------------------------------------  --------------------------------------------------------  ----------------------------------------------\n",
      "placeholder    x                                    x                                                       ()                                                        {}\n",
      "call_function  x_dict                               <function get_dict at 0x7f3574ccd940>                   (x,)                                                      {}\n",
      "call_method    x__review                            get                                                     (x_dict, 'review', None)                                  {}\n",
      "placeholder    edge_index                           edge_index                                              ()                                                        {}\n",
      "call_function  edge_index_dict                      <function get_dict at 0x7f3574ccd940>                   (edge_index,)                                             {}\n",
      "call_method    edge_index__review__net_rsr__review  get                                                     (edge_index_dict, ('review', 'net_rsr', 'review'), None)  {}\n",
      "call_method    edge_index__review__net_rtr__review  get                                                     (edge_index_dict, ('review', 'net_rtr', 'review'), None)  {}\n",
      "call_method    edge_index__review__net_rur__review  get                                                     (edge_index_dict, ('review', 'net_rur', 'review'), None)  {}\n",
      "call_function  dropout__review                      <function dropout at 0x7f35d19f3ce0>                    (x__review,)                                              {'p': 0.6, 'training': True, 'inplace': False}\n",
      "call_module    conv1__review1                       conv1.review__net_rsr__review                           (dropout__review, edge_index__review__net_rsr__review)    {}\n",
      "call_module    conv1__review2                       conv1.review__net_rtr__review                           (dropout__review, edge_index__review__net_rtr__review)    {}\n",
      "call_module    conv1__review3                       conv1.review__net_rur__review                           (dropout__review, edge_index__review__net_rur__review)    {}\n",
      "call_function  conv1__review_1                      <built-in method add of type object at 0x7f36bbe655e0>  (conv1__review1, conv1__review2)                          {}\n",
      "call_function  conv1__review                        <built-in method add of type object at 0x7f36bbe655e0>  (conv1__review3, conv1__review_1)                         {}\n",
      "call_function  elu__review                          <function elu at 0x7f35d1a00400>                        (conv1__review,)                                          {'alpha': 1.0, 'inplace': False}\n",
      "call_function  dropout_1__review                    <function dropout at 0x7f35d19f3ce0>                    (elu__review,)                                            {'p': 0.6, 'training': True, 'inplace': False}\n",
      "call_module    conv2__review1                       conv2.review__net_rsr__review                           (dropout_1__review, edge_index__review__net_rsr__review)  {}\n",
      "call_module    conv2__review2                       conv2.review__net_rtr__review                           (dropout_1__review, edge_index__review__net_rtr__review)  {}\n",
      "call_module    conv2__review3                       conv2.review__net_rur__review                           (dropout_1__review, edge_index__review__net_rur__review)  {}\n",
      "call_function  conv2__review_1                      <built-in method add of type object at 0x7f36bbe655e0>  (conv2__review1, conv2__review2)                          {}\n",
      "call_function  conv2__review                        <built-in method add of type object at 0x7f36bbe655e0>  (conv2__review3, conv2__review_1)                         {}\n",
      "output         output                               output                                                  ({'review': conv2__review},)                              {}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x, edge_index):\n",
      "    x_dict = torch_geometric_nn_to_hetero_transformer_get_dict(x);  x = None\n",
      "    x__review = x_dict.get('review', None);  x_dict = None\n",
      "    edge_index_dict = torch_geometric_nn_to_hetero_transformer_get_dict(edge_index);  edge_index = None\n",
      "    edge_index__review__net_rsr__review = edge_index_dict.get(('review', 'net_rsr', 'review'), None)\n",
      "    edge_index__review__net_rtr__review = edge_index_dict.get(('review', 'net_rtr', 'review'), None)\n",
      "    edge_index__review__net_rur__review = edge_index_dict.get(('review', 'net_rur', 'review'), None);  edge_index_dict = None\n",
      "    dropout__review = torch.nn.functional.dropout(x__review, p = 0.6, training = True, inplace = False);  x__review = None\n",
      "    conv1__review1 = self.conv1.review__net_rsr__review(dropout__review, edge_index__review__net_rsr__review)\n",
      "    conv1__review2 = self.conv1.review__net_rtr__review(dropout__review, edge_index__review__net_rtr__review)\n",
      "    conv1__review3 = self.conv1.review__net_rur__review(dropout__review, edge_index__review__net_rur__review);  dropout__review = None\n",
      "    conv1__review_1 = torch.add(conv1__review1, conv1__review2);  conv1__review1 = conv1__review2 = None\n",
      "    conv1__review = torch.add(conv1__review3, conv1__review_1);  conv1__review3 = conv1__review_1 = None\n",
      "    elu__review = torch.nn.functional.elu(conv1__review, alpha = 1.0, inplace = False);  conv1__review = None\n",
      "    dropout_1__review = torch.nn.functional.dropout(elu__review, p = 0.6, training = True, inplace = False);  elu__review = None\n",
      "    conv2__review1 = self.conv2.review__net_rsr__review(dropout_1__review, edge_index__review__net_rsr__review);  edge_index__review__net_rsr__review = None\n",
      "    conv2__review2 = self.conv2.review__net_rtr__review(dropout_1__review, edge_index__review__net_rtr__review);  edge_index__review__net_rtr__review = None\n",
      "    conv2__review3 = self.conv2.review__net_rur__review(dropout_1__review, edge_index__review__net_rur__review);  dropout_1__review = edge_index__review__net_rur__review = None\n",
      "    conv2__review_1 = torch.add(conv2__review1, conv2__review2);  conv2__review1 = conv2__review2 = None\n",
      "    conv2__review = torch.add(conv2__review3, conv2__review_1);  conv2__review3 = conv2__review_1 = None\n",
      "    return {'review': conv2__review}\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaifeng/Documents/gnn_project/LAGAT/LAGAT/lib/python3.12/site-packages/torch_geometric/nn/fx.py:132: UserWarning: Found function 'dropout' with keyword argument 'training'. During FX tracing, this will likely be baked in as a constant value. Consider replacing this function by a module to properly encapsulate its training flag.\n",
      "  warnings.warn(f\"Found function '{node.name}' with keyword \"\n",
      "/home/kaifeng/Documents/gnn_project/LAGAT/LAGAT/lib/python3.12/site-packages/torch_geometric/nn/fx.py:132: UserWarning: Found function 'dropout_1' with keyword argument 'training'. During FX tracing, this will likely be baked in as a constant value. Consider replacing this function by a module to properly encapsulate its training flag.\n",
      "  warnings.warn(f\"Found function '{node.name}' with keyword \"\n"
     ]
    }
   ],
   "source": [
    "# from torch_geometric.nn.models import GAT\n",
    "\n",
    "# Creating a model instance covering heterogeneity\n",
    "# model = GAT(in_channels=32 ,hidden_channels=32, num_layers=2, out_channels=2)\n",
    "\n",
    "model = GAT(in_channels=32 ,hidden_channels=32, heads=2, out_channels=2)\n",
    "model = to_hetero(model, data.metadata(), aggr='sum', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [00:30<00:00,  9.81it/s, Loss=0.3751]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.feature_dict, data.edge_index_dict)\n",
    "\n",
    "    train_mask = data['review'].train_mask.to(device)\n",
    "    label = data['review'].label.to(device)\n",
    "\n",
    "    logits = out['review'][train_mask.bool()]\n",
    "    targets = label[train_mask.bool()].long()\n",
    "\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "n_epochs = 300\n",
    "progress_bar = tqdm(range(n_epochs), desc='Training')\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    loss = train()\n",
    "    progress_bar.set_postfix({'Loss': f'{loss:.4f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Metrics ---\n",
      "F1 Score: 0.4602, AUC: 0.7145, AP: 0.3041\n",
      "--- Validation Metrics ---\n",
      "F1 Score: 0.4629, AUC: 0.7165, AP: 0.2910\n",
      "--- Test Metrics ---\n",
      "F1 Score: 0.4615, AUC: 0.7146, AP: 0.2907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': (0.4601920648439106, 0.7145413732230615, 0.30406614726013415),\n",
       " 'val': (0.4629499766245909, 0.7164793646571422, 0.2910083500944018),\n",
       " 'test': (0.46150989110166585, 0.7145653532011383, 0.2906736540234121)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.feature_dict, data.edge_index_dict)\n",
    "        scores = torch.softmax(out['review'], dim=1)  # Convert logits to probabilities\n",
    "\n",
    "    labels = data['review'].label.cpu()\n",
    "    pred = scores.argmax(dim=1).cpu()\n",
    "\n",
    "    def calc_metrics(target_mask):\n",
    "        mask_indices = target_mask.cpu()\n",
    "        masked_labels = labels[mask_indices.bool()]\n",
    "        masked_pred = pred[mask_indices.bool()]\n",
    "        masked_scores = scores[mask_indices.bool()][:, 1].cpu()\n",
    "\n",
    "        f1 = f1_score(masked_labels, masked_pred, average='macro')\n",
    "        try:\n",
    "            auc = roc_auc_score(masked_labels, masked_scores)\n",
    "            ap = average_precision_score(masked_labels, masked_scores)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            auc, ap = float('nan'), float('nan')  # In case of an exception (like only one class present), return NaN\n",
    "        return f1, auc, ap\n",
    "\n",
    "    train_metrics = calc_metrics(data['review'].train_mask)\n",
    "    val_metrics = calc_metrics(data['review'].val_mask)\n",
    "    test_metrics = calc_metrics(data['review'].test_mask)\n",
    "\n",
    "    print('--- Training Metrics ---')\n",
    "    print(f'F1 Score: {train_metrics[0]:.4f}, AUC: {train_metrics[1]:.4f}, AP: {train_metrics[2]:.4f}')\n",
    "    \n",
    "    print('--- Validation Metrics ---')\n",
    "    print(f'F1 Score: {val_metrics[0]:.4f}, AUC: {val_metrics[1]:.4f}, AP: {val_metrics[2]:.4f}')\n",
    "    \n",
    "    print('--- Test Metrics ---')\n",
    "    print(f'F1 Score: {test_metrics[0]:.4f}, AUC: {test_metrics[1]:.4f}, AP: {test_metrics[2]:.4f}')\n",
    "\n",
    "    return {'train': train_metrics, 'val': val_metrics, 'test': test_metrics}\n",
    "\n",
    "_ = test(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is multi-step label leakage in training, might not learn anything, might learn something\n",
    "# This way can avoid using ego network (which cannot be run efficiently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, reset, zeros\n",
    "from torch_geometric.utils import (add_remaining_self_loops, add_self_loops,\n",
    "                                   remove_self_loops, softmax)\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "\n",
    "class LAGATConvLayer(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_labels,\n",
    "                 label_embedding_dim,\n",
    "                 heads=1,\n",
    "                 concat=True,\n",
    "                 negative_slope=0.2,\n",
    "                 dropout=0,\n",
    "                 bias=False,\n",
    "                 **kwargs):\n",
    "        super(LAGATConvLayer, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        # add label related params\n",
    "        self.num_labels = num_labels\n",
    "        self.label_embedding_dim = label_embedding_dim\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels,\n",
    "                                             heads * out_channels))\n",
    "        # Add learnable label embedding \n",
    "        self.label_embs = Parameter(torch.Tensor(self.num_labels, self.label_embedding_dim))\n",
    "\n",
    "        # add Label Embedding into attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels + self.label_embedding_dim))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, label_mask, size=None):\n",
    "        \"\"\"\"\"\"\n",
    "        if size is None and torch.is_tensor(x):\n",
    "            edge_index, _ = remove_self_loops(edge_index)\n",
    "            edge_index, _ = add_self_loops(edge_index,\n",
    "                                           num_nodes=x.size(self.node_dim))\n",
    "        if torch.is_tensor(x):\n",
    "            x = torch.matmul(x, self.weight)\n",
    "        else:\n",
    "            x = (None if x[0] is None else torch.matmul(x[0], self.weight),\n",
    "                None if x[1] is None else torch.matmul(x[1], self.weight))\n",
    "\n",
    "        # print(f\"x: {x.shape}, edge_index: {edge_index.shape}, label_mask: {label_mask.shape}\")\n",
    "        return self.propagate(edge_index, size=size, x=x, label_mask=label_mask)\n",
    "\n",
    "    def message(self, edge_index_i, edge_index_j, x_i, x_j, size_i, label_mask):\n",
    "        # Compute attention coefficients.\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "        # index and expanded label_emb to be concatenated\n",
    "        loop_edge_mask = edge_index_i == edge_index_j\n",
    "        label_j = label_mask[edge_index_j]\n",
    "        # Replace labels for loop edges with 0 (index for self.label_embs[0])\n",
    "        label_j = torch.where(loop_edge_mask, torch.zeros_like(label_j), label_j)\n",
    "        label_emb = self.label_embs[label_j]\n",
    "        label_emb = label_emb.unsqueeze(1).repeat(1, self.heads, 1)\n",
    "        if x_i is None:\n",
    "            alpha = (torch.cat([x_j, label_emb], dim=-1) * self.att[:, :, self.out_channels:]).sum(dim=-1)\n",
    "        else:\n",
    "            x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "            alpha = (torch.cat([x_i, x_j, label_emb], dim=-1) * self.att).sum(dim=-1)\n",
    "\n",
    "\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, edge_index_i, num_nodes=size_i)\n",
    "\n",
    "        # Sample attention coefficients stochastically.\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        # return (x_j * alpha.view(-1, self.heads, 1)).view(-1, self.heads*self.out_channels)\n",
    "        output = (x_j * alpha.view(-1, self.heads, 1)).view(-1, self.heads*self.out_channels)\n",
    "        # print(f\"x_i: {x_i.shape}, x_j: {x_j.shape}, label_mask: {label_mask.shape}, alpha: {alpha.shape}, output: {output.shape}\")\n",
    "        return output\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # if self.concat is True:\n",
    "        #     aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n",
    "        # else:\n",
    "        #     aggr_out = aggr_out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAGAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
