{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch import nn\n",
    "from torch_geometric.utils import from_dgl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.fraud_dataset as fraud_dataset\n",
    "\n",
    "DATASET_NAME = \"yelp\"\n",
    "TRAIN_SIZE = 0.4\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_SEED = 42\n",
    "FORCE_RELOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method HeteroData.metadata of HeteroData(\n",
       "  review={\n",
       "    test_mask=[45954],\n",
       "    val_mask=[45954],\n",
       "    train_mask=[45954],\n",
       "    label=[45954],\n",
       "    feature=[45954, 32],\n",
       "  },\n",
       "  (review, net_rsr, review)={ edge_index=[2, 6805486] },\n",
       "  (review, net_rtr, review)={ edge_index=[2, 1147232] },\n",
       "  (review, net_rur, review)={ edge_index=[2, 98630] }\n",
       ")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data = fraud_dataset.FraudDataset(\n",
    "    DATASET_NAME, \n",
    "    train_size=TRAIN_SIZE, \n",
    "    val_size=VAL_SIZE, \n",
    "    random_seed=RANDOM_SEED, \n",
    "    force_reload=FORCE_RELOAD\n",
    ")\n",
    "graph = fraud_data[0]\n",
    "\n",
    "data = from_dgl(graph)\n",
    "data.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2388)\n",
      "tensor(0.0412)\n",
      "tensor(0.7200)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def mask_label(data, observed_pct=1):\n",
    "    # Ensure observed_pct is a value between 0 and 1\n",
    "    assert 0 <= observed_pct <= 1, \"observed_pct must be between 0 and 1\"\n",
    "    \n",
    "    # Create a copy of the labels to modify\n",
    "    label_mask = data[\"review\"].label.clone()\n",
    "    unknown_encoding = -1\n",
    "\n",
    "    # Mask all validation and test labels\n",
    "    label_mask[data[\"review\"].val_mask.bool()] = unknown_encoding\n",
    "    label_mask[data[\"review\"].test_mask.bool()] = unknown_encoding\n",
    "\n",
    "    # Identify the indices of the training data\n",
    "    train_indices = data[\"review\"].train_mask.nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "    # Calculate the number of training labels to mask\n",
    "    num_train_labels = train_indices.size(0)\n",
    "    num_to_mask = int((1 - observed_pct) * num_train_labels)\n",
    "\n",
    "    # Randomly select indices to mask\n",
    "    mask_indices = train_indices[torch.randperm(num_train_labels)[:num_to_mask]]\n",
    "    label_mask[mask_indices] = unknown_encoding\n",
    "\n",
    "    return label_mask + 1\n",
    "\n",
    "# Example usage\n",
    "masked_labels = mask_label(data, 0.7)\n",
    "# Count the ratio of each label\n",
    "print((masked_labels == 1).float().mean())  # Prints the fraction of labels that are original class 0\n",
    "print((masked_labels == 2).float().mean())  # Prints the fraction of labels that are original class 1\n",
    "print((masked_labels == 0).float().mean())  # Prints the fraction of labels that are masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, reset, zeros\n",
    "from torch_geometric.utils import (add_remaining_self_loops, add_self_loops,\n",
    "                                   remove_self_loops, softmax)\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "\n",
    "class LAGATConvLayer(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_labels,\n",
    "                 label_embedding_dim,\n",
    "                 heads=1,\n",
    "                 concat=True,\n",
    "                 negative_slope=0.2,\n",
    "                 dropout=0,\n",
    "                 bias=False,\n",
    "                 **kwargs):\n",
    "        super(LAGATConvLayer, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        # add label related params\n",
    "        self.num_labels = num_labels\n",
    "        self.label_embedding_dim = label_embedding_dim\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels,\n",
    "                                             heads * out_channels))\n",
    "        # Add learnable label embedding \n",
    "        self.label_embs = Parameter(torch.Tensor(num_labels, 1, label_embedding_dim))\n",
    "\n",
    "        # add Label Embedding into attention calculation\n",
    "        self.att_src = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_dst = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_lab = Parameter(torch.Tensor(1, heads, label_embedding_dim))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.att_dst)\n",
    "        glorot(self.att_src)\n",
    "        glorot(self.att_lab)\n",
    "        glorot(self.label_embs)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, label_mask, size=None):\n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        if size is None and torch.is_tensor(x):\n",
    "            edge_index, _ = remove_self_loops(edge_index)\n",
    "            edge_index, _ = add_self_loops(edge_index,\n",
    "                                        num_nodes=x.size(self.node_dim))\n",
    "        \n",
    "        if torch.is_tensor(x):\n",
    "            x_src = x_dst = torch.matmul(x, self.weight).view(-1, H, C)\n",
    "        else:\n",
    "            x_src, x_dst = x\n",
    "            x_src = torch.matmul(x_src, self.weight).view(-1, H, C)\n",
    "            if x_dst is not None:\n",
    "                x_dst = torch.matmul(x_dst, self.weight).view(-1, H, C)\n",
    "\n",
    "        alpha_src = (x_src * self.att_src).sum(-1)\n",
    "        alpha_dst = None if x_dst is None else (x_dst * self.att_dst).sum(-1)\n",
    "        alpha_lab = (self.label_embs * self.att_lab).sum(-1)\n",
    "        alpha = (alpha_src, alpha_dst)\n",
    "\n",
    "        x = (x_src.view(-1, H*C), x_dst.view(-1, H*C))\n",
    "        \n",
    "        return self.propagate(edge_index, size=size, x=x, \n",
    "                            alpha=alpha, alpha_lab=alpha_lab, label_mask=label_mask)\n",
    "\n",
    "    def message(self, edge_index_i, edge_index_j, x_j, alpha_i, \n",
    "                alpha_j, size_i, label_mask, alpha_lab):\n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        # Initialize alpha\n",
    "        if alpha_i is not None:\n",
    "            alpha = alpha_j + alpha_i\n",
    "        else:\n",
    "            alpha = alpha_j\n",
    "        \n",
    "        # Handle label attention\n",
    "        loop_edge_mask = edge_index_i == edge_index_j\n",
    "        label_j = torch.index_select(label_mask, 0, edge_index_j)\n",
    "        label_j = torch.where(loop_edge_mask, torch.zeros_like(label_j), label_j)\n",
    "        \n",
    "        alpha_label = torch.index_select(alpha_lab, 0, label_j)\n",
    "        alpha = alpha + alpha_label\n",
    "        \n",
    "        # Apply attention mechanisms\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, edge_index_i, num_nodes=size_i)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Ensure correct shapes for final multiplication\n",
    "        x_j = x_j.view(-1, H, C)\n",
    "        alpha = alpha.view(-1, H, 1)\n",
    "        \n",
    "        return (x_j * alpha).view(-1, H * C)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # if self.concat is True:\n",
    "        #     aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n",
    "        # else:\n",
    "        #     aggr_out = aggr_out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lagatconv import LAGATConv\n",
    "\n",
    "\n",
    "class GATWithLabels(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_labels, label_embedding_dim, heads=1, dropout=0.6):\n",
    "        super(GATWithLabels, self).__init__()\n",
    "        self.conv1 = LAGATConv(in_channels, hidden_channels, num_labels, label_embedding_dim, heads=heads, concat=True, dropout=dropout, bias=False)\n",
    "        self.conv2 = LAGATConv(hidden_channels * heads, out_channels, num_labels, label_embedding_dim, heads=1, concat=True, bias=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, label_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, label_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, label_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name         target                                args                                  kwargs\n",
      "-------------  -----------  ------------------------------------  ------------------------------------  ----------------------------------------------\n",
      "placeholder    x            x                                     ()                                    {}\n",
      "placeholder    edge_index   edge_index                            ()                                    {}\n",
      "placeholder    label_index  label_index                           ()                                    {}\n",
      "call_function  dropout      <function dropout at 0x7f95f69e3ce0>  (x,)                                  {'p': 0.6, 'training': True, 'inplace': False}\n",
      "call_module    conv1        conv1                                 (dropout, edge_index, label_index)    {}\n",
      "call_function  elu          <function elu at 0x7f95f69f0400>      (conv1,)                              {'alpha': 1.0, 'inplace': False}\n",
      "call_function  dropout_1    <function dropout at 0x7f95f69e3ce0>  (elu,)                                {'p': 0.6, 'training': True, 'inplace': False}\n",
      "call_module    conv2        conv2                                 (dropout_1, edge_index, label_index)  {}\n",
      "output         output       output                                (conv2,)                              {}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x, edge_index, label_index):\n",
      "    dropout = torch.nn.functional.dropout(x, p = 0.6, training = True, inplace = False);  x = None\n",
      "    conv1 = self.conv1(dropout, edge_index, label_index);  dropout = None\n",
      "    elu = torch.nn.functional.elu(conv1, alpha = 1.0, inplace = False);  conv1 = None\n",
      "    dropout_1 = torch.nn.functional.dropout(elu, p = 0.6, training = True, inplace = False);  elu = None\n",
      "    conv2 = self.conv2(dropout_1, edge_index, label_index);  dropout_1 = edge_index = label_index = None\n",
      "    return conv2\n",
      "    \n",
      "opcode         name                                 target                                                  args                                                                           kwargs\n",
      "-------------  -----------------------------------  ------------------------------------------------------  -----------------------------------------------------------------------------  ----------------------------------------------\n",
      "placeholder    x                                    x                                                       ()                                                                             {}\n",
      "call_function  x_dict                               <function get_dict at 0x7f955ac25620>                   (x,)                                                                           {}\n",
      "call_method    x__review                            get                                                     (x_dict, 'review', None)                                                       {}\n",
      "placeholder    edge_index                           edge_index                                              ()                                                                             {}\n",
      "call_function  edge_index_dict                      <function get_dict at 0x7f955ac25620>                   (edge_index,)                                                                  {}\n",
      "call_method    edge_index__review__net_rsr__review  get                                                     (edge_index_dict, ('review', 'net_rsr', 'review'), None)                       {}\n",
      "call_method    edge_index__review__net_rtr__review  get                                                     (edge_index_dict, ('review', 'net_rtr', 'review'), None)                       {}\n",
      "call_method    edge_index__review__net_rur__review  get                                                     (edge_index_dict, ('review', 'net_rur', 'review'), None)                       {}\n",
      "placeholder    label_index                          label_index                                             ()                                                                             {}\n",
      "call_function  label_index_dict                     <function get_dict at 0x7f955ac25620>                   (label_index,)                                                                 {}\n",
      "call_method    label_index__review                  get                                                     (label_index_dict, 'review', None)                                             {}\n",
      "call_function  dropout__review                      <function dropout at 0x7f95f69e3ce0>                    (x__review,)                                                                   {'p': 0.6, 'training': True, 'inplace': False}\n",
      "call_module    conv1__review1                       conv1.review__net_rsr__review                           (dropout__review, edge_index__review__net_rsr__review, label_index__review)    {}\n",
      "call_module    conv1__review2                       conv1.review__net_rtr__review                           (dropout__review, edge_index__review__net_rtr__review, label_index__review)    {}\n",
      "call_module    conv1__review3                       conv1.review__net_rur__review                           (dropout__review, edge_index__review__net_rur__review, label_index__review)    {}\n",
      "call_function  conv1__review_1                      <built-in method add of type object at 0x7f96ae0655e0>  (conv1__review1, conv1__review2)                                               {}\n",
      "call_function  conv1__review                        <built-in method add of type object at 0x7f96ae0655e0>  (conv1__review3, conv1__review_1)                                              {}\n",
      "call_function  elu__review                          <function elu at 0x7f95f69f0400>                        (conv1__review,)                                                               {'alpha': 1.0, 'inplace': False}\n",
      "call_function  dropout_1__review                    <function dropout at 0x7f95f69e3ce0>                    (elu__review,)                                                                 {'p': 0.6, 'training': True, 'inplace': False}\n",
      "call_module    conv2__review1                       conv2.review__net_rsr__review                           (dropout_1__review, edge_index__review__net_rsr__review, label_index__review)  {}\n",
      "call_module    conv2__review2                       conv2.review__net_rtr__review                           (dropout_1__review, edge_index__review__net_rtr__review, label_index__review)  {}\n",
      "call_module    conv2__review3                       conv2.review__net_rur__review                           (dropout_1__review, edge_index__review__net_rur__review, label_index__review)  {}\n",
      "call_function  conv2__review_1                      <built-in method add of type object at 0x7f96ae0655e0>  (conv2__review1, conv2__review2)                                               {}\n",
      "call_function  conv2__review                        <built-in method add of type object at 0x7f96ae0655e0>  (conv2__review3, conv2__review_1)                                              {}\n",
      "output         output                               output                                                  ({'review': conv2__review},)                                                   {}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x, edge_index, label_index):\n",
      "    x_dict = torch_geometric_nn_to_hetero_transformer_get_dict(x);  x = None\n",
      "    x__review = x_dict.get('review', None);  x_dict = None\n",
      "    edge_index_dict = torch_geometric_nn_to_hetero_transformer_get_dict(edge_index);  edge_index = None\n",
      "    edge_index__review__net_rsr__review = edge_index_dict.get(('review', 'net_rsr', 'review'), None)\n",
      "    edge_index__review__net_rtr__review = edge_index_dict.get(('review', 'net_rtr', 'review'), None)\n",
      "    edge_index__review__net_rur__review = edge_index_dict.get(('review', 'net_rur', 'review'), None);  edge_index_dict = None\n",
      "    label_index_dict = torch_geometric_nn_to_hetero_transformer_get_dict(label_index);  label_index = None\n",
      "    label_index__review = label_index_dict.get('review', None);  label_index_dict = None\n",
      "    dropout__review = torch.nn.functional.dropout(x__review, p = 0.6, training = True, inplace = False);  x__review = None\n",
      "    conv1__review1 = self.conv1.review__net_rsr__review(dropout__review, edge_index__review__net_rsr__review, label_index__review)\n",
      "    conv1__review2 = self.conv1.review__net_rtr__review(dropout__review, edge_index__review__net_rtr__review, label_index__review)\n",
      "    conv1__review3 = self.conv1.review__net_rur__review(dropout__review, edge_index__review__net_rur__review, label_index__review);  dropout__review = None\n",
      "    conv1__review_1 = torch.add(conv1__review1, conv1__review2);  conv1__review1 = conv1__review2 = None\n",
      "    conv1__review = torch.add(conv1__review3, conv1__review_1);  conv1__review3 = conv1__review_1 = None\n",
      "    elu__review = torch.nn.functional.elu(conv1__review, alpha = 1.0, inplace = False);  conv1__review = None\n",
      "    dropout_1__review = torch.nn.functional.dropout(elu__review, p = 0.6, training = True, inplace = False);  elu__review = None\n",
      "    conv2__review1 = self.conv2.review__net_rsr__review(dropout_1__review, edge_index__review__net_rsr__review, label_index__review);  edge_index__review__net_rsr__review = None\n",
      "    conv2__review2 = self.conv2.review__net_rtr__review(dropout_1__review, edge_index__review__net_rtr__review, label_index__review);  edge_index__review__net_rtr__review = None\n",
      "    conv2__review3 = self.conv2.review__net_rur__review(dropout_1__review, edge_index__review__net_rur__review, label_index__review);  dropout_1__review = edge_index__review__net_rur__review = label_index__review = None\n",
      "    conv2__review_1 = torch.add(conv2__review1, conv2__review2);  conv2__review1 = conv2__review2 = None\n",
      "    conv2__review = torch.add(conv2__review3, conv2__review_1);  conv2__review3 = conv2__review_1 = None\n",
      "    return {'review': conv2__review}\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaifeng/Documents/gnn_project/LAGAT/LAGAT/lib/python3.12/site-packages/torch_geometric/nn/fx.py:132: UserWarning: Found function 'dropout' with keyword argument 'training'. During FX tracing, this will likely be baked in as a constant value. Consider replacing this function by a module to properly encapsulate its training flag.\n",
      "  warnings.warn(f\"Found function '{node.name}' with keyword \"\n",
      "/home/kaifeng/Documents/gnn_project/LAGAT/LAGAT/lib/python3.12/site-packages/torch_geometric/nn/fx.py:132: UserWarning: Found function 'dropout_1' with keyword argument 'training'. During FX tracing, this will likely be baked in as a constant value. Consider replacing this function by a module to properly encapsulate its training flag.\n",
      "  warnings.warn(f\"Found function '{node.name}' with keyword \"\n"
     ]
    }
   ],
   "source": [
    "# Creating a model instance covering heterogeneity\n",
    "model = GATWithLabels(in_channels=32, hidden_channels=32, label_embedding_dim=32, num_labels=3, out_channels=2, heads=2)\n",
    "model = to_hetero(model, data.metadata(), aggr='sum', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [00:54<00:00,  5.47it/s, Loss=0.3977]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "label_mask = {\"review\": masked_labels}\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "label_mask = {key: value.to(device) for key, value in label_mask.items()}\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.feature_dict, data.edge_index_dict, label_mask)\n",
    "\n",
    "    train_mask = data['review'].train_mask.to(device)\n",
    "    label = data['review'].label.to(device)\n",
    "\n",
    "    logits = out['review'][train_mask.bool()]\n",
    "    targets = label[train_mask.bool()].long()\n",
    "\n",
    "    # Perform the loss computation\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "n_epochs = 300\n",
    "progress_bar = tqdm(range(n_epochs), desc='Training')\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    loss = train()\n",
    "    progress_bar.set_postfix({'Loss': f'{loss:.4f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Metrics ---\n",
      "F1 Score: 0.4602, AUC: 0.6290, AP: 0.2271\n",
      "--- Validation Metrics ---\n",
      "F1 Score: 0.4629, AUC: 0.6322, AP: 0.2114\n",
      "--- Test Metrics ---\n",
      "F1 Score: 0.4609, AUC: 0.6322, AP: 0.2262\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.feature_dict, data.edge_index_dict, label_mask)\n",
    "        scores = torch.softmax(out['review'], dim=1)  # Convert logits to probabilities\n",
    "\n",
    "    labels = data['review'].label.cpu()\n",
    "    pred = scores.argmax(dim=1).cpu()\n",
    "\n",
    "    def calc_metrics(target_mask):\n",
    "        mask_indices = target_mask.cpu()\n",
    "        masked_labels = labels[mask_indices.bool()]\n",
    "        masked_pred = pred[mask_indices.bool()]\n",
    "        masked_scores = scores[mask_indices.bool()][:, 1].cpu()\n",
    "\n",
    "        f1 = f1_score(masked_labels, masked_pred, average='macro')\n",
    "        try:\n",
    "            auc = roc_auc_score(masked_labels, masked_scores)\n",
    "            ap = average_precision_score(masked_labels, masked_scores)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            auc, ap = float('nan'), float('nan')  # In case of an exception (like only one class present), return NaN\n",
    "        return f1, auc, ap\n",
    "\n",
    "    train_metrics = calc_metrics(data['review'].train_mask)\n",
    "    val_metrics = calc_metrics(data['review'].val_mask)\n",
    "    test_metrics = calc_metrics(data['review'].test_mask)\n",
    "\n",
    "    print('--- Training Metrics ---')\n",
    "    print(f'F1 Score: {train_metrics[0]:.4f}, AUC: {train_metrics[1]:.4f}, AP: {train_metrics[2]:.4f}')\n",
    "    \n",
    "    print('--- Validation Metrics ---')\n",
    "    print(f'F1 Score: {val_metrics[0]:.4f}, AUC: {val_metrics[1]:.4f}, AP: {val_metrics[2]:.4f}')\n",
    "    \n",
    "    print('--- Test Metrics ---')\n",
    "    print(f'F1 Score: {test_metrics[0]:.4f}, AUC: {test_metrics[1]:.4f}, AP: {test_metrics[2]:.4f}')\n",
    "\n",
    "    return {'train': train_metrics, 'val': val_metrics, 'test': test_metrics}\n",
    "\n",
    "_ = test(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn import Parameter\n",
    "# from torch_geometric.nn.conv import MessagePassing\n",
    "# from torch_geometric.nn.inits import glorot, reset, zeros\n",
    "# from torch_geometric.utils import (add_remaining_self_loops, add_self_loops,\n",
    "#                                    remove_self_loops, softmax)\n",
    "# from torch_scatter import scatter_add\n",
    "\n",
    "\n",
    "# class LAGATConvLayer(MessagePassing):\n",
    "#     def __init__(self,\n",
    "#                  in_channels,\n",
    "#                  out_channels,\n",
    "#                  num_labels,\n",
    "#                  label_embedding_dim,\n",
    "#                  heads=1,\n",
    "#                  concat=True,\n",
    "#                  negative_slope=0.2,\n",
    "#                  dropout=0,\n",
    "#                  bias=False,\n",
    "#                  **kwargs):\n",
    "#         super(LAGATConvLayer, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         # add label related params\n",
    "#         self.num_labels = num_labels\n",
    "#         self.label_embedding_dim = label_embedding_dim\n",
    "#         self.heads = heads\n",
    "#         self.concat = concat\n",
    "#         self.negative_slope = negative_slope\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         self.weight = Parameter(torch.Tensor(in_channels,\n",
    "#                                              heads * out_channels))\n",
    "#         # Add learnable label embedding \n",
    "#         self.label_embs = Parameter(torch.Tensor(self.num_labels, self.label_embedding_dim))\n",
    "\n",
    "#         # add Label Embedding into attention calculation\n",
    "#         self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels + self.label_embedding_dim))\n",
    "\n",
    "#         if bias and concat:\n",
    "#             self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "#         elif bias and not concat:\n",
    "#             self.bias = Parameter(torch.Tensor(out_channels))\n",
    "#         else:\n",
    "#             self.register_parameter('bias', None)\n",
    "\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         glorot(self.weight)\n",
    "#         glorot(self.att)\n",
    "#         zeros(self.bias)\n",
    "\n",
    "#     def forward(self, x, edge_index, label_mask, size=None):\n",
    "#         \"\"\"\"\"\"\n",
    "#         if size is None and torch.is_tensor(x):\n",
    "#             edge_index, _ = remove_self_loops(edge_index)\n",
    "#             edge_index, _ = add_self_loops(edge_index,\n",
    "#                                            num_nodes=x.size(self.node_dim))\n",
    "#         if torch.is_tensor(x):\n",
    "#             x = torch.matmul(x, self.weight)\n",
    "#         else:\n",
    "#             x = (None if x[0] is None else torch.matmul(x[0], self.weight),\n",
    "#                 None if x[1] is None else torch.matmul(x[1], self.weight))\n",
    "\n",
    "#         # print(f\"x: {x.shape}, edge_index: {edge_index.shape}, label_mask: {label_mask.shape}\")\n",
    "#         return self.propagate(edge_index, size=size, x=x, label_mask=label_mask)\n",
    "\n",
    "#     def message(self, edge_index_i, edge_index_j, x_i, x_j, size_i, label_mask):\n",
    "#         # Compute attention coefficients.\n",
    "#         x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "#         # index and expanded label_emb to be concatenated\n",
    "#         loop_edge_mask = edge_index_i == edge_index_j\n",
    "#         label_j = label_mask[edge_index_j]\n",
    "#         # Replace labels for loop edges with 0 (index for self.label_embs[0])\n",
    "#         label_j = torch.where(loop_edge_mask, torch.zeros_like(label_j), label_j)\n",
    "#         label_emb = self.label_embs[label_j]\n",
    "#         label_emb = label_emb.unsqueeze(1).repeat(1, self.heads, 1)\n",
    "#         if x_i is None:\n",
    "#             alpha = (torch.cat([x_j, label_emb], dim=-1) * self.att[:, :, self.out_channels:]).sum(dim=-1)\n",
    "#         else:\n",
    "#             x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "#             alpha = (torch.cat([x_i, x_j, label_emb], dim=-1) * self.att).sum(dim=-1)\n",
    "\n",
    "\n",
    "#         alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "#         alpha = softmax(alpha, edge_index_i, num_nodes=size_i)\n",
    "\n",
    "#         # Sample attention coefficients stochastically.\n",
    "#         alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "#         # return (x_j * alpha.view(-1, self.heads, 1)).view(-1, self.heads*self.out_channels)\n",
    "#         output = (x_j * alpha.view(-1, self.heads, 1)).view(-1, self.heads*self.out_channels)\n",
    "#         # print(f\"x_i: {x_i.shape}, x_j: {x_j.shape}, label_mask: {label_mask.shape}, alpha: {alpha.shape}, output: {output.shape}\")\n",
    "#         return output\n",
    "\n",
    "#     def update(self, aggr_out):\n",
    "#         # if self.concat is True:\n",
    "#         #     aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n",
    "#         # else:\n",
    "#         #     aggr_out = aggr_out.mean(dim=1)\n",
    "\n",
    "#         if self.bias is not None:\n",
    "#             aggr_out = aggr_out + self.bias\n",
    "#         return aggr_out\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "#                                              self.in_channels,\n",
    "#                                              self.out_channels, self.heads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAGAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
